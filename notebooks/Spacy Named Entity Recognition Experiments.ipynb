{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overhead-hydrogen",
   "metadata": {},
   "source": [
    "# Spacy Named Entity Recognition Experiments\n",
    "This notebook is for practice with Spacy's named entity recognition.  The aim being to formulate the process for taking the text created from the PDF and recognizing the required features to extract and save in the Database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-artwork",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stainless-miracle",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (51.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.56.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/erbun/.local/share/virtualenvs/Lambda-School-Labs-human-rights-first-asyl-iMyelADK/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=6c1055fe53a15f4c07523fd42c0b3ebe3c1b5aac3001697681d49a4c762e3dd3\n",
      "  Stored in directory: /private/var/folders/vg/jzyp1_n94pq55jdvv6mtktw40000gn/T/pip-ephem-wheel-cache-43mup_6f/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greater-amplifier",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aa6fbfe2f6a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Imports for Doc Analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# nlp = spacy.load('en_core_web_sm')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# Imports for Doc Analysis\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Imports for uploading PDF and converting it to text\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path, convert_from_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elegant-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(path):\n",
    "    \"\"\"\n",
    "    Takes the path to a PDF file, creates a PIL image, and then reads through the image\n",
    "    convertin the images into text.\n",
    "    \n",
    "    INPUT: path\n",
    "    \n",
    "    RETURNS: Text Object\n",
    "    \"\"\"\n",
    "    fulltext = ''\n",
    "    \n",
    "    # Read in the pdf to the PIL image\n",
    "    pil_image = convert_from_path(path)\n",
    "    \n",
    "    # Iterate through PIL image and convert each page to text\n",
    "    text = [str(pytesseract.image_to_string(image)) for image in pil_image]\n",
    "\n",
    "    # Iterate through the raw text and format returns\n",
    "    for t in text:\n",
    "        fulltext += t\n",
    "        \n",
    "    return fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "naughty-expense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"NORP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-offset",
   "metadata": {},
   "source": [
    "### Extracting basic entities\n",
    "The function below will start to extract entities out of PDF files whose path is fed to it.  The goal for right now is to streamline the process from path to entities in order to make comparisons between different pdf entities to look for commonalities in trying to extract relevant features with a high degree of accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fifteen-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(pdf_path):\n",
    "    \"\"\"\n",
    "    Function that takes in a path to a pdf file, uses the get_text() function to convert \n",
    "    the file to text, and then uses SpaCy to extract and return relevant entities.\n",
    "    \n",
    "    INPUT: path to pdf file\n",
    "    \n",
    "    RETURNS: list of people, orgs, and other entities as relevant for testing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the pdf file to text, and then fit it to the NLP Model\n",
    "    text = get_text(pdf_path)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Setup lists to append entities to\n",
    "    people = set()\n",
    "    orgs = set()\n",
    "    norps = set()\n",
    "    gpes = set()\n",
    "    \n",
    "    # Iterate through the entities, compare for relevant entities, and append them to \n",
    "    # the relevant list\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            # print('Name: ', ent.text)  # Printing lines for pattern testing\n",
    "            people.add(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            # print('ORG: ', ent.text)\n",
    "            orgs.add(ent.text)\n",
    "        elif ent.label_ == \"NORP\":\n",
    "            # print('NORP: ', ent.text)\n",
    "            norps.add(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            # print('GPE: ', ent.text)\n",
    "            gpes.add(ent.text)\n",
    "            \n",
    "    # Return the entity lists\n",
    "    print('Entity Extraction Complete')\n",
    "    return people, orgs, norps, gpes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "electrical-handbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Extraction Complete\n"
     ]
    }
   ],
   "source": [
    "people, orgs, norps, gpes = get_entities('test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "million-trader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "{'U.S.', 'Virginia', 'Mexico', 'the United States', 'Maryland', 'the United\\n\\nStates', 'Falls Church', 'Baltimore', 'MD 21201'}\n",
      "14\n",
      "{'joWOeIT MMM', 'File Nos', 'Charles K.\\n', 'Sweeney', 'Maureen A. Sweeney', 'Ferino Sanchez Seltik', 'Jennifer Piateski', 'Lopez-Mendoza', 'DAVID W. CROSLAND', 'Donna Carr', 'Denna Cane', 'Jennifer BE', 'Maureen A.', 'John'}\n",
      "31\n",
      "{'Cite', 'DAVILA', 'I&N', 'Free State Reporting, Inc.', 'Leesburg Pike', 'an Appellate Court', 'Office of the Clerk', 'Contractor', 'Sony', 'Executive Office for Immigration Review\\n\\nBoard of Immigration Appeals', 'Government', 'DHS/ICE Office of Chief Counsel', 'OARD', 'Section C', 'U.S. Department of Justice Decision', 'Matter of Toro', 'FERINO', 'Executive Office for Immigration Review\\n\\n', 'the Board of Immigration Appeals', 'Immigration Judges de', 'The Board', 'the Supreme Court', 'the Executive Office', 'Board', 'U.S. Department of Justice', 'the Immigration Court', 'United States Immigration', 'the\\n\\nImmigration and Nationality Act', 'Esquire\\n\\nUniversity of Maryland Immigration Clinic', 'the Executive Office for Immigration Review', 'Court'}\n",
      "3\n",
      "{'Cuban', 'FERINO', 'CUBAN'}\n"
     ]
    }
   ],
   "source": [
    "print(len(gpes))\n",
    "print(gpes)\n",
    "print(len(people))\n",
    "print(people)\n",
    "print(len(orgs))\n",
    "print(orgs)\n",
    "print(len(norps))\n",
    "print(norps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "virgin-layout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Extraction Complete\n"
     ]
    }
   ],
   "source": [
    "people, orgs, norps, gpes = get_entities('appeal_test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dated-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{'TX', 'Houston', 'Northpoint Drive', 'Virginia', 'the United States', 'Guatemala', 'remand', 'Falls Church', 'Copan', 'Honduras'}\n",
      "18\n",
      "{'Jose', 'Cynthia L. Crosby', 'Sheridan Green Law PLLC', 'Keily Janeth', 'Esquire\\n\\n', 'ANALYSIS', 'Gonzales', 'AXXX XXX 957', 'Chen', 'Sheridan G. Green', 'Tab D', 'Nimmo Bhagat', 'Sheridan Gary DHSI/ICE Office', 'QJ', 'Gavino Pineda', 'Maura Suyapa Varela-Erazo', 'Tortus', 'Honduras'}\n",
      "28\n",
      "{'Office of the Clerk EJ', 'I&N', 'the U.S. Department of State', 'The Fifth Circuit', 'The Department of Homeland Security', 'Executive Office for Immigration Review', 'CREDIBILITY\\n\\n', 'Bi Department', 'Whenthe-Court', 'AXXX XXX 957', 'Homeland Security', 'United', 'U.S. Department of Justice Decision', 'Department', 'INA Section', 'DHS', 'Board of Immigration Appeals', 'the Board of Immigration Appeals', 'the Country Report', 'GE', 'the United Nations', 'U.S. Department of Justice\\n\\nExecutive Office for Immigration Review\\n\\n \\n\\n', 'Board', '|-589', 'Torture', 'IJ', 'XZ', 'Court'}\n",
      "2\n",
      "{'ty', 'Guatemalan'}\n"
     ]
    }
   ],
   "source": [
    "print(len(gpes))\n",
    "print(gpes)\n",
    "print(len(people))\n",
    "print(people)\n",
    "print(len(orgs))\n",
    "print(orgs)\n",
    "print(len(norps))\n",
    "print(norps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "local-woman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Extraction Complete\n"
     ]
    }
   ],
   "source": [
    "people, orgs, norps, gpes = get_entities('test1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "natural-assessment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "{'MA', 'Boston', 'Virginia', 'Florida', 'Falls Church', 'Denna'}\n",
      "9\n",
      "{'LAUDELINO', 'Mark D.', 'P.O. Box 8728', 'Miller', 'Neil P.\\n\\n', 'Donna Carr', 'Gwendylan Tregerman', 'Mark D. Cooper', 'Esq'}\n",
      "18\n",
      "{'Cite', 'Board', 'U.S. Department of Justice Decision', 'U.S. Department of Justice', 'JOAO', 'Executive Office for Immigration Review\\n\\nBoard of Immigration Appeals', 'SNjay', 'DHS', 'Office of the Clerk\\n\\n \\n\\nCooper', 'The Department of Homeland Security', 'Joao Silva Laudelino', 'DHS/ICE Office of Chief Counsel', 'Falls Church', 'the Board of Immigration Appeals', 'Leesburg Pike', 'Enclosure\\n\\n', 'JOAO SILVA LAUDELINO', 'Executive Office for Immigration Review'}\n",
      "0\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(len(gpes))\n",
    "print(gpes)\n",
    "print(len(people))\n",
    "print(people)\n",
    "print(len(orgs))\n",
    "print(orgs)\n",
    "print(len(norps))\n",
    "print(norps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-chester",
   "metadata": {},
   "source": [
    "### Refining the entities to extract specific information\n",
    "We now have a straightforward process for extracting out lists of entities from a path to a pdf file.  However, we have the issue of getting many entities that are similar in nature, and we need to determine heuristics for determining which specific entities match for feature extraction. (e.g. We can pull out several names, but need a workable heuristic to reliably determine who is who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-campus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
